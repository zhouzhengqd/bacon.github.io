<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>This is my paper title</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="BACON: Bayesian Optimal Condensation Framework for Dataset Distillation." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<hr>
	<br>
	<center>
		<td><img class="round" style="width:800px" src="./resources/logo.png"/></td>
		<table align=center width=600px>
			<table align="center" width="100%" style="border-collapse: collapse;">
				<!-- 第一行作者 -->
				<tr>
					<td align="center" width="12%">
						<a href="https://zhouzhengqd.github.io/" style="font-size: 20px; text-decoration: none;">Zheng Zhou<sup>1</sup></a>
					</td>
					<td align="center" width="12%">
						<a href="https://shi.buaa.edu.cn/09698/zh_CN/index.htm" style="font-size: 20px; text-decoration: none;">Hongbo Zhao<sup>1</sup></a>
					</td>
					<td align="center" width="15%" style="white-space: nowrap;">
						<a href="https://sites.google.com/view/guangliangcheng" style="font-size: 20px; text-decoration: none;">Guangliang Cheng<sup>2</sup></a>
					</td>
					<td align="center" width="12%">
						<a href="https://lxtgh.github.io/" style="font-size: 20px; text-decoration: none;">Xiangtai Li<sup>3</sup></a>
					</td>
			
					<td align="center" width="12%">
						<a href="https://scholar.google.com/citations?user=SwGcxzMAAAAJ&hl=en" style="font-size: 20px; text-decoration: none;">Shuchang Lyu*<sup>1</sup></a>
					</td>
					<td align="center" width="12%">
						<a href="https://shi.buaa.edu.cn/fengwenquan/zh_CN/index/132879/list/" style="font-size: 20px; text-decoration: none;">Wenquan Feng<sup>1</sup></a>
					</td>
					<td align="center" width="12%">
						<a href="https://shi.buaa.edu.cn/07297/zh_CN/index.htm" style="font-size: 20px; text-decoration: none;">Qi Zhao<sup>1</sup></a>
					</td>
				</tr>
			
				<!-- 机构信息 -->
				<tr>
					<td colspan="7" align="center" style="font-size: 20px; line-height: 1.6;">
						<sup>1</sup>Beihang University, 
						<sup>2</sup>University of Liverpool, 
						<sup>3</sup>Nanyang Technological University
					</td>
				</tr>				
			</table>
			<!-- 脚注 -->
			<p style="font-size: 18px; text-align: center;">* Corresponding Author</p>
			<table align="center" width="50%" style="border-collapse: collapse;">
				<tr>
					<td style="text-align: center; width: 20%; padding: 10px;">
						<span style="font-size:20px"><a href='https://arxiv.org/pdf/2406.01112'>[Paper]</a></span>
					</td>
					<td style="text-align: center; width: 20%; padding: 10px;">
						<span style="font-size:20px"><a href='https://github.com/zhouzhengqd/BACON.git'>[GitHub]</a></span>
					</td>
					<td style="text-align: center; width: 20%; padding: 10px;">
						<span style="font-size:20px"><a href='https://share.multcloud.link/share/f496af96-494a-4815-a7c9-e93cd95ecdd1'>[Distilled Dataset]</a></span>
					</td>
				</tr>
			</table>
			
		</table>
	</center>


	<hr>

	<table align="center" width="850px">
		<tr>
			<td width="260px">
				<center>
					<figure>
						<img class="round" style="width:800px" src="./resources/overview.png"/>
						<figcaption style="font-size: 16px; text-align: left; color: #555;">
							<strong>Figure 1:</strong> Comparison of BACON and existing DD methods: (a) Traditional methods align gradients and distributions on original and synthetic datasets. (b) BACON transforms DD into a Bayesian optimization task, generating synthetic images using likelihood and prior probabilities. 
						</figcaption>
					</figure>
				</center>
			</td>
		</tr>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p style="background-color: #f0f0f0; padding: 15px;">
					<strong>Abstract:</strong> Dataset Distillation (DD) aims to distill knowledge from extensive datasets into more compact ones while preserving performance on the test set, thereby reducing storage costs and training expenses. However, existing methods often suffer from computational intensity, particularly exhibiting suboptimal performance with large dataset sizes due to the lack of a robust theoretical framework for analyzing the DD problem. To address these challenges, we propose the <u><strong>BA</strong></u>yesian optimal <u><strong>CON</strong></u>densation framework (<u><strong>BACON</strong></u>), which is the first work to introduce the Bayesian theoretical framework to the literature of DD. This framework provides theoretical support for enhancing the performance of DD. Furthermore, BACON formulates the DD problem as the minimization of the expected risk function in joint probability distributions using the Bayesian framework. Additionally, by analyzing the expected risk function for optimal condensation, we derive a numerically feasible lower bound based on specific assumptions, providing an approximate solution for BACON. We validate BACON across several datasets, demonstrating its superior performance compared to existing state-of-the-art methods. For instance, under the IPC-10 setting, BACON achieves a 3.46% accuracy gain over the IDM method on the CIFAR-10 dataset and a 3.10% gain on the TinyImageNet dataset. Our extensive experiments confirm the effectiveness of BACON and its seamless integration with existing methods, thereby enhancing their performance for the DD task. Code and distilled datasets are available at <a href="https://github.com/zhouzhengqd/BACON" target="_blank"><u><strong>BACON</strong></u></a>.
				</p>
				
				
			</td>
		</tr>
	</table>
	<br>
	
	<hr>
	
	<center><h1>Motivation</h1></center>

	<p>
		<strong>Dataset Distillation (DD)</strong> aims to reduce dataset size while maintaining test set performance, but existing methods struggle with large datasets and lack a solid theoretical foundation. Current approaches are computationally intensive and often perform poorly on complex tasks, particularly with large-scale datasets.
	</p>
	<ul>
		<li><i>How can we effectively formulate the DD problem?</i></li>
		<li><i>What is the theoretical lower bound of optimal condensation?</i></li>
	  </ul>
	
	<p>
		To address these challenges, we introduce the <strong>Bayesian Optimal Condensation Framework (BACON)</strong>. This is the first Bayesian approach to DD, providing a clear theoretical foundation and a numerically feasible solution. BACON formulates DD as a minimization problem within Bayesian joint distributions, significantly improving performance, especially on large datasets like CIFAR-10 and TinyImageNet.
	</p>
	Key Benefits of BACON:
	<ul>
		<li><i><strong>Strong Theoretical Foundation:</strong> Leveraging Bayesian optimization to guide the DD process.</i></li>
		<li><i><strong>Improved Performance:</strong> Outperforms existing methods with higher accuracy on benchmark datasets.</i></li>
		<li><i><strong>Scalability:</strong> Efficiently handles large datasets and integrates seamlessly with existing systems.</i></li>
	  </ul>
	<hr>
	<center><h1>Bayesian Optimal Condensation Framework</h1></center>
	<tr>
		<td width="260px">
			<center>
				<figure>
					<img class="round" style="width:800px" src="./resources/method.png"/>
					<figcaption style="font-size: 16px; text-align: left; color: #555;">
						<strong>Figure 2:</strong> Illustration of BACON: The neural network outputs a distribution from both synthetic and real datasets. BACON formulates this distribution as a Bayesian optimal condensation risk function and derives its optimal solution using Bayesian principles. 
					</figcaption>
				</figure>
			</center>
		</td>
	</tr>
	  <p>As illustrated in Figure 2, BACON uses a joint probability model, based on Bayesian theory, to optimize dataset distillation tasks. The framework calculates the optimal synthetic dataset by deriving a condensation risk function and applies approximation techniques for efficient solution computation.</p>
	  <ul>
		<li><i>Optimal Condensation Risk Function: A derived expected risk function optimizes dataset representation, ensuring the synthetic dataset retains key features of the original.</i></li>
		<li><i>Approximation Methods: Uses Monte Carlo sampling and other methods to efficiently approximate the optimal solution.</i></li>
	  </ul>
	  <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Math Theorem Example</title>
		<script type="text/javascript" async
		  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>
	</head>
	<body>
		<h1>Theorem: Bayesian Optimal Condensation Risk Function</h1>
	
		<p>The optimal embedding feature of synthetic image $z_{\tilde{x}}^*$ can be computed as follows </p>
	
		<p>
			\[
			z_{\tilde{x}}^* = argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} \left[\log p(z_{\tilde{x}} | z_x) + \log p(z_x)\right] dz_x.
			\]
		</p>
	
		<p><strong>Proof:</strong> By leveraging Bayes' rule and Jensen's inequality, we derive the function as follows:</p>
	
		<p>
			\[
			z_{\tilde{x}}^* = argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} p(z_x | z_{\tilde{x}}) dz_x
			\]
		</p>
		<p>
			\[
			= argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} \frac{p(z_{\tilde{x}} | z_x) p(z_x)}{p(z_{\tilde{x}})} dz_x
			\]
		</p>
		<p>
			\[
			= argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} \underbrace{p(z_{\tilde{x}} | z_x) p(z_x)}_{\text{Bayesian formula}} dz_x
			\]
		</p>
		<p>
			\[
			= argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \left[ \log \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} p(z_{\tilde{x}} | z_x) p(z_x) dz_x \right]
			\]
		</p>
		<p>
			\[
			\geq argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} \log \left[ p(z_{\tilde{x}} | z_x) p(z_x) \right] dz_x
			\quad \text{(by Jensen's inequality)}
			\]
		</p>
		<p>
			\[
			= argmax_{z_{\tilde{x}} \in \mathcal{D}_S} \int_{\mathcal{B}(z_{\tilde{x}}, \epsilon)} \left[\log p(z_{\tilde{x}} | z_x) + \log p(z_x)\right] dz_x
			\]
		</p>
		<h1>Assumptions and Loss Function</h1>
	
		<h2>Assumption 1: Likelihood Conforming Gaussian</h2>
		<p>
			To estimate the log-likelihood \( \log p(z_{\tilde{x}} | z_{x_i}) \), we make the assumption that \( p(z_{\tilde{x}} | z_{x_i}) \) conforms to a Gaussian distribution. In this distribution, \( \sigma_x^2 \) represents the variance and \( z_{x_i} \) represents the mean. It is denoted as:
		</p>
		<p>
			\[
			p(z_{\tilde{x}} | z_{x_i}) \sim \mathcal{N}(z_{x_i}, \sigma_{xi}^2 I)
			\]
		</p>
	
		<h2>Assumption 2: Prior Distribution Approximation with TV Extension</h2>
		<p>
			The Total Variation (TV) and CLIP operation are incorporated as distribution priors to represent \( \log p(z_{x_i}) \). The CLIP operation constrains the probability within the bound of \( [0,1] \). In contrast to their study, we extend the TV from a pixel-wise approach to a distribution-wise approach, which is also referred to as the total variation of probability distribution measures.
		</p>
	
		<h2>Loss Functions</h2>
		<p>
			Under Assumption 1 and Assumption 2, we mitigate computational costs by using mini-batch sampling instead of Monte Carlo methods. Mini-batch sampling, which divides the dataset into smaller subsets, is more convenient for training and reduces computational overhead, whereas Monte Carlo methods require more extensive computational resources. Consequently, we divide the following equation into three separate loss terms:
		</p>
	
		<p>
			\[
			\mathcal{L}_{\text{LH}} = -\frac{1}{k}\sum_{i=1}^{k}\left[\frac{1}{2}\log(2 \pi \sigma_{x_i}) +\frac{1}{2\sigma_{x_i}^2}\Vert z_{\tilde{x}}- z_{x_i}\Vert_2^2\right]
			\]
		</p>
	
		<p>
			\[
			\mathcal{L}_{\text{TV}} = \frac{1}{k}\sum_{i=1}^{k}\left(\frac{1}{2} \Vert z_{\tilde{x}} - z_{x_i}\Vert_1\right)
			\]
		</p>
	
		<p>
			\[
			\mathcal{L}_{\text{CLIP}} =  \frac{1}{k}\sum_{i=1}^{k}\left[\frac{z_{\tilde{x}} - z_{x_i}}{\sigma_{x_i}} - \text{CLIP}\left(\frac{z_{\tilde{x}} - z_{x_i}}{\sigma_{x_i}}, 0, 1\right)\right]^2
			\]
		</p>
	
		<h3>Overall Loss Function</h3>
		<p>
			To summarize, the overall loss function of BACON integrates the three loss terms above. The combined loss function is defined as:
		</p>
	
		<p>
			\[
			\mathcal{L}_{\text{TOTAL}} = \mathcal{L}_{\text{LH}} + \lambda \mathcal{L}_{\text{TV}} + (1-\lambda)\mathcal{L}_{\text{CLIP}}
			\]
		</p>
	
		<p>
			where the hyperparameter \( \lambda \) serves as the weighting factor for the total loss function and is adjustable. By tuning \( \lambda \), we can customize the loss function to optimize performance.
		</p>
	</body>


	<hr>

	<center><h1>Results</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td width="260px">
					<center>
						<figure>
							<img class="round" style="width:800px" src="./resources/results_1.png"/>
							<figcaption style="font-size: 16px; text-align: left; color: #555;">
								<strong>Table 1:</strong> Comparison with previous coreset selection and dataset condensation methods: Like most state-of-the-art methods, we evaluate our method on six datasets (MNIST, Fashion-MNIST, SVHN, CIFAR-10/100, TinyImageNet) with different numbers of synthetic images per class (IPC). The ``Ratio(%)'' represents the condensed images' ratio to the entire training set. For reference, ``Full Set'' indicates the accuracy of the trained model on the complete training set. It's important to note that DD and LD employ different architectures, specifically LeNet for MNIST and AlexNet for CIFAR-10. Meanwhile, the remaining methods all utilize ConvNet. 
							</figcaption>
						</figure>
					</center>
				</td>
			</tr>
				<tr>
				<td width="260px">
					<center>
						<figure>
							<img class="round" style="width:800px" src="./resources/results_2.png"/>
							<figcaption style="font-size: 16px; text-align: left; color: #555;">
								<strong>Figure 3:</strong> Performance comparison with BACON, IDM, and DM across varying training steps on the CIFAR-10/100 datasets: The blue line with white circles represents our proposed BACON, the orange line with white circles represents IDM, and the green line with white circles represents DM. All synthetic images are generated using the CIFAR-10/100 datasets across training steps from 0 to 20000 with IPC-1, IPC-10, and IPC-50, respectively. 
							</figcaption>
						</figure>
					</center>
				</td>
			</tr>
		</center>
	</table>
	<hr>
	<table align=center width=450px>
		<center><h1>Visulization</h1></center>
		<tr>
			<td><img class="round" style="width:800px" src="./resources/mnist.png"/></td>
		</tr>
		<tr>
			<td><img class="round" style="width:800px" src="./resources/f-mnist.png"/></td>
		</tr>
		<tr>
			<td><img class="round" style="width:800px" src="./resources/svhn.png"/></td>
		</tr>
		<tr>
			<td><img class="round" style="width:800px" src="./resources/cifar-100.png"/></td>
		</tr>
		<tr>
			<td><img class="round" style="width:800px" src="./resources/visulization.png"/></td>
		</tr>
	</table>
	<br>

	<hr>
	<table align=center width=450px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Zheng Zhou, Hongbo Zhao, Guangliang Cheng, Xiangtai Li, Shuchang Lyu, Wenquan Feng, and Qi Zhao<br>
				<b>BACON: Bayesian Optimal Condensation Framework for Dataset Distillation.</b><br>
				In submission, 2024.<br>
				(hosted on <a href="https://arxiv.org/pdf/2406.01112">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<div style="text-align: center; width: 900px; margin: 0 auto;">
		<h1>Acknowledgements</h1>
		<p>We gratefully acknowledge the contributors of DC-bench and IDM, as our code builds upon their work. 
			You can find their repositories here: 
			<a href="https://github.com/justincui03/dc_benchmark?tab=readme-ov-file" target="_blank">DC-bench</a> and 
			<a href="https://github.com/uitrbn/IDM" target="_blank">IDM</a>.
		</p>
	</div>
	

<br>
</body>
</html>

