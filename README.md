<p align="center">
<img src="resources/logo.png" width="100%" class="center" alt="pipeline"/>
</p>

Website of BEARD: Benchmarking the Adversarial Robustness for Dataset Distillation

**Website**: [https://zhouzhengqd.github.io/bacon.website/](https://zhouzhengqd.github.io/bacon.website/).

**Code**: [https://github.com/zhouzhengqd/BACON.git](https://github.com/zhouzhengqd/BACON.git).

**Paper:** [https://arxiv.org/pdf/2406.01112](https://arxiv.org/pdf/2406.01112).



## Abstract

> **Abstract** Dataset Distillation (DD) reduces dataset size while maintaining test set performance, helping to cut storage and training costs. Current DD methods struggle with large datasets and lack a solid theoretical foundation. To address this, we introduce the <u>**BA**</u>yesian Optimal <u>**CON**</u>densation Framework (<u>**BACON**</u>), the first Bayesian approach to DD. BACON formulates DD as a minimization problem in Bayesian joint distributions and derives a numerically feasible lower bound. Our experiments show that BACON outperforms state-of-the-art methods, with significant accuracy improvements on CIFAR-10 and TinyImageNet. BACON seamlessly integrates with existing systems and boosts DD performance. Code and distilled datasets are available at [BACON](https://github.com/zhouzhengqd/BACON).
